{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Bringing in our Data\n",
    "\n",
    "The following cells should be starting to look familiar to you - I'm not going to annotate them here.  They a) bring in our libraries, b) read in our .csv file, c) rename our variables, and d) create our dummy variables for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call our libraries; note, we are adding some libraries to our notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import t\n",
    "#from datascience import *\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df=pd.read_csv(\"sf_acs_pums_p.csv\")\n",
    "person_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df.rename(columns={\"sporder\":\"p_number\",\n",
    "                          \"wagp\":\"wages\",\n",
    "                          \"occp\":\"occupation\",\n",
    "                          \"rac1p\":\"race\",\n",
    "                          \"fhisp\":\"hispanic\"}, inplace=True)\n",
    "person_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingunit_df=pd.read_csv(\"sf_acs_pums_h.csv\")\n",
    "housingunit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingunit_df.rename(columns={\"bld\":\"buildingsize\",\n",
    "                               \"rntp\":\"rent\",\n",
    "                               \"ybl\":\"yearbuilt\"}, inplace=True)\n",
    "housingunit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Merging Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Figuring Out What You're Merging\n",
    "\n",
    "We generally merge datasets based on a ***key*** or common variable.  In this case, both datasets have a serialno, which is the identifier associated with the housing unit that the census sampled in conducting their ACS survey.  When you look at the HousingUnit dataset, you see each serialno is unique.  But in the person dataset, you have four rows of the serial number 2013000000253 - one for each person in the household, which is indicated by the person number. \n",
    "\n",
    "The options for merging are:\n",
    "\n",
    "a ***one to one merge***: this is when both files have only one observation per key variable, for example, both datafiles contain aggregate data at the tract FIPS level, and you want to merge by the census tract FIPS.\n",
    "\n",
    "a ***one to many merge***: this is when you want to merge a dataset with one observation per key value to one with more observations.  A good example here is if I want to attach the data on the housing unit (**one** record per unit) to every person in the dataset (**many** people per unit). \n",
    "\n",
    "a ***many to one merge***: this is when you want to merge a dataset with many observations per key value to one with just a single observation. A good example here is if I want to attach the data on the people (**many** records per unit) to every housing unit in the dataset (**one** per unit). \n",
    "\n",
    "Let's look at the code for all three and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merging One to One\n",
    "\n",
    "First, let's look at the easiest scenario - we just want to merge one to one!  In our practice data, I'm going to drop everyone but the head of household, so I have the same number of observations of my key variable for my housing units and my person file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I drop the rows (index) where the person variable in the person_df is not equal to 1\n",
    "# I'm going to rename the dataframe so I can use the \"raw\" person data later.\n",
    "head_df=person_df.drop(person_df[person_df['p_number'] != 1].index)\n",
    "head_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums_person_unit_df=pd.merge(head_df, housingunit_df, on=\"serialno\")\n",
    "pums_person_unit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One to Many Merge\n",
    "\n",
    "This time, I want to add the \"housing unit\" information to every person in my dataset.  (For example, if I want to know if children are more likely to live in single-family versus larger apartment buildings, I might want to know what kind of building every child in the dataset lives in.)\n",
    "\n",
    "Honestly, merging is the first function I've found in Python that's easier than in SAS or STATA!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personwithhousing_df=pd.merge(person_df, housingunit_df, on=\"serialno\")\n",
    "personwithhousing_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 OK - What about Many to One?\n",
    "\n",
    "In general, we want to \"group by\" first - for example, I want to know how many people are in the housing unit.  But let's say I really just want to attach all the people to each housing unit.  I can just swap the order of the merge.  But, in reality, this just looks like my one to many merge above, it's just that the first columns contain the building information rather than the person information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_allpeeps_df=pd.merge(housingunit_df, person_df, on=\"serialno\")\n",
    "housing_allpeeps_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Grouping \"by\" to Get New Data\n",
    "\n",
    "More often, we want to group a number of observations by a certain characteristic.  For example, in this case, I might want to know the number of people in the household.  Others of you will want to count how many evictions or traffic accidents are happening in a census tract or zip code.  Once I merge the data, I can \"aggregate\" the information using the same \"group by\" concept we used when we were calculating descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this code, I start by grouping my data by unit, or serialno\n",
    "#Alternately, you could do this by fips tract\n",
    "df_by_unit=housing_allpeeps_df.groupby(\"serialno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I'm going to tell Python how I want to aggregate the various columns,\n",
    "# as well as what I want the new variable to be named\n",
    "housing_allpeeps_df[\"hh_size\"] = df_by_unit[\"p_number\"].transform(\"count\")\n",
    "housing_allpeeps_df[\"total_wages\"] =df_by_unit[\"wages\"].transform(\"sum\")\n",
    "housing_allpeeps_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Getting Rid of Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, if I want to remove the duplicates, I can run my code to just select the head of household\n",
    "# Notice the conditional statement (boolean) in this code. Take a moment to talk through this line of code with\n",
    "# a neighbor, and describe what we're telling Python with each command.\n",
    "\n",
    "pums_data=housing_allpeeps_df[housing_allpeeps_df['p_number'] == 1].copy()\n",
    "pums_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Order of Operations\n",
    "\n",
    "I generally clean and create dummies, then make new categories of each dataset before I merge them. A best practice is to first write out what steps you need to do before you do them, and then think about the logical approach to coding. And, I often make mistakes and have to go back, but luckily I have all my code so it goes faster as I make fixes to get to the analysis I want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Dropping Values\n",
    "\n",
    "The .drop() function allows us to drop columns, rows, or values from a dataframe. We can also specify conditional statements like we did up above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Dropping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This lets us know if there are any null values\n",
    "print( pums_data.isnull().values.any() ) \n",
    "\n",
    "# This shows us the total number of missing values for each variable\n",
    "print( pums_data.isnull().sum() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '.dropna' function drops values that Python recognizes as missing.\n",
    "pums_data.dropna(inplace=True)\n",
    "\n",
    "print( pums_data.isnull().sum() ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dropping a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code drops the wages column, so we don't get confused about which wage variable to use in this dataframe.\n",
    "# To drop more than one column, simply list the column labes in square brackets: ['wages', 'occupation', etc.]\n",
    "pums_data.drop('wages', axis=1, inplace=True)\n",
    "\n",
    "pums_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Dropping a Row (Using a Conditional Statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's say we want to know more about household size. Let's take a quick look at that variable.\n",
    "pums_data['hh_size'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Woah - 14 is a lot of people! Let's take a closer look to see if this is an outlier.\n",
    "\n",
    "# Sneak peak of the code in the optional notebook we've put together with more resources. Check it out!\n",
    "\n",
    "# Here we are defining a function (called hh_size_histogram) which runs all of the code we tell it to. \n",
    "# Defining our own functions just makes it easier to get the same output later. \n",
    "# We're also using a couple new libraries to plot a histogram.\n",
    "\n",
    "def hh_size_histogram ():\n",
    "    sns.set()\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(\"Household Size\")\n",
    "    plt.xticks(rotation=35)\n",
    "    sns.countplot(pums_data['hh_size'])\n",
    "    plt.ylabel(\"Number of Housing Units\")\n",
    "    plt.xlabel(\"Household Size\")\n",
    "    plt.show()\n",
    "    \n",
    "hh_size_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and drop household sizes over 10 people. \n",
    "# Keep in mind there are many ways to get the same result in Python. What would be another option here?\n",
    "\n",
    "pums_data.drop( pums_data[pums_data['hh_size'] > 10].index, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at this variable in our dataframe to see what this did.\n",
    "\n",
    "pums_data['hh_size'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_size_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Data Binning\n",
    "Sometimes it can be useful to change a continuous variable into a categorical one. This is called 'binning.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the distribution of our total wages\n",
    "pums_data[\"total_wages\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums_data[\"total_wages\"].quantile([0,.2,.4,.6,.8,.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining bin cutoffs\n",
    "wage_bins = ( 0, 25000, 50000, 100000, 150000, 1008000 )\n",
    "             # Far left number is minimum, far right number is maximum value\n",
    "\n",
    "# Defining bin labels\n",
    "bin_labels = ( 'Very Low Wage', 'Low Wage', 'Moderate Wage', 'High Wage', 'Very High Warnings')\n",
    "\n",
    "# Defining a new variable 'monthly earnings'\n",
    "pums_data['wage_brackets'] = pd.cut( pums_data['total_wages'], wage_bins, labels=bin_labels )\n",
    "# Tip: Hold down shift and tab, and click on the command 'cut' to see more instructions from Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pums_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
