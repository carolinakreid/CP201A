{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Welcome back!!  The Jupyter interface should now be feeling familiar, and you should have an intuition for how Python processes cells, the difference between a code cell and a markdown cell, and some basic operations.\n",
    "\n",
    "In lab today, we're going to work on three concepts: panda dataframes and indexing (these are confusing so we're going to review them and provide stronger guidance on how to think about them), using conditional statements, and writing more complex formulas.\n",
    "\n",
    "By the end of today's lab, you should feel empowered to:\n",
    "- upload your own data\n",
    "- rename variables\n",
    "- make a copy of your dataframe, selecting specific rows and/or columns you want to work with\n",
    "- create a dummy variable\n",
    "- write formulas to create new variables\n",
    "\n",
    "The other code in this notebook is designed to help you advance faster and use Python to analyze ACS data in a more systemic way than in Excel.  But, Excel is powerful too, so feel free to use that tool to revise Assignment 1 if that feels better to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries\n",
    "\n",
    "Let's begin by importing our libraries. How might you do that on your own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datascience import *\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importing our Data\n",
    "\n",
    "If you brought one of your .csv datasets, you can upload it now, and work with that dataset throughout this lab.  Otherwise, we will be working with the \"geographic\" mobility dataset I introduced a few weeks ago.  It includes data on renters versus owners and when they moved into their current house.\n",
    "\n",
    "If you are working on your own dataset, you'll be replacing my code with your own in every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to work with the downloaded raw .csv file from the ACS.  But you can always choose to edit your ACS file first and then bring it into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify datatypes so FIPS numbers, etc. read in properly\n",
    "variable_types = {'Geo.id': 'str', 'GEO.id2': 'str'}\n",
    "#variable_types in this case is known as a \"dictionary\" - thanks to the curly brackets\n",
    "#it tells Python that every time it sees \"Geo.id\", make that variable a string\n",
    "\n",
    "#read in my csv file, saying it is comma delimited, I want index row 0 to be the header, \n",
    "# I want to skip index row 1 (the long variable names), and I want to assign the datatypes\n",
    "#I've outlined above.\n",
    "\n",
    "pd.read_csv('ACS_17_5YR_B07013_with_ann.csv', delimiter = ',', header=[0], skiprows=[1], dtype=variable_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 909 rows in the data, and each row has 39 columns. The first column is the index. The index is used to identify the position of the data, but it is not an actual column. It looks like the read_csv function in Pandas read our file properly. However, we haven’t saved any data to memory so we can work with it. We need to assign the data to an object. We can create a new object with a name by assigning a value to it using \"=\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_df=pd.read_csv('ACS_17_5YR_B07013_with_ann.csv', delimiter = ',', header=[0], skiprows=[1], dtype=variable_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the code to see the data here, ideally just five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I want to know the kind of object I've just created, I can type \"type(name)\"\n",
    "type(mobility_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I want to see whether my datatypes imported correctly, I can type (name.dtypes)\n",
    "mobility_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Renaming our Variables\n",
    "\n",
    "I am now going to write some code to rename my columns into something that is more intuitive for me to type.  I can do this in the .csv first, but I like this approach because then I can run it again for different geographies/different years in the future.  See my example Excel sheet for my approach to doing this efficiently!\n",
    "\n",
    "As in the above example, I am going to create a dictionary file (with the curly brackets), and assign the new name to each old name.  I will use the function \"rename\".  The inplace=\"True\" command tells Python to overwrite the dataframe with the new names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_df.rename(columns={\"GEO.id\":\"fullfips\",\"GEO.id2\":\"fips\",\n",
    "\"GEO.display-label\":\"label\",\n",
    "\"HD01_VD01\":\"tot\",\n",
    "\"HD02_VD01\":\"tot_moe\",\n",
    "\"HD01_VD03\":\"owner\",\n",
    "\"HD02_VD03\":\"owner_moe\",\n",
    "\"HD01_VD04\":\"renter\",\n",
    "\"HD02_VD04\":\"renter_moe\",\n",
    "\"HD01_VD05\":\"tot_samehouse\",\n",
    "\"HD02_VD05\":\"tot_samehouse_moe\",\n",
    "\"HD01_VD06\":\"owner_samehouse\",\n",
    "\"HD02_VD06\":\"owner_samehouse_moe\",\n",
    "\"HD01_VD07\":\"renter_samehouse\",\n",
    "\"HD02_VD07\":\"renter_samehouse_moe\",\n",
    "\"HD01_VD08\":\"tot_cty\",\n",
    "\"HD02_VD08\":\"tot_cty_moe\",\n",
    "\"HD01_VD09\":\"owner_cty\",\n",
    "\"HD02_VD09\":\"owner_cty_moe\",\n",
    "\"HD01_VD10\":\"renter_cty\",\n",
    "\"HD02_VD10\":\"renter_cty_moe\",\n",
    "\"HD01_VD11\":\"tot_dcty\",\n",
    "\"HD02_VD11\":\"tot_dcty_moe\",\n",
    "\"HD01_VD12\":\"owner_dcty\",\n",
    "\"HD02_VD12\":\"owner_dcty_moe\",\n",
    "\"HD01_VD13\":\"renter_dcty\",\n",
    "\"HD02_VD13\":\"renter_dcty_moe\",\n",
    "\"HD01_VD14\":\"tot_state\",\n",
    "\"HD02_VD14\":\"tot_state_moe\",\n",
    "\"HD01_VD15\":\"owner_state\",\n",
    "\"HD02_VD15\":\"owner_state_moe\",\n",
    "\"HD01_VD16\":\"renter_state\",\n",
    "\"HD02_VD16\":\"renter_state_moe\",\n",
    "\"HD01_VD17\":\"tot_abroad\",\n",
    "\"HD02_VD17\":\"tot_abroad_moe\",\n",
    "\"HD01_VD18\":\"owner_abroad\",\n",
    "\"HD02_VD18\":\"owner_abroad_moe\",\n",
    "\"HD01_VD19\":\"renter_abroad\",\n",
    "\"HD02_VD19\":\"renter_abroad_moe\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good practice to check and make sure your code ran correctly.  Check to see if your column names showed up the way you wanted, and that you're dataset is in good shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Working with our Dataset in Python\n",
    "\n",
    "We're going to step back for a minute and spend a bit more time explaining how dataframe objects work in Python.  We often want to work with subsets of our data (e.g., just certain columns or variables).  In Python, this is referred to as \"indexing\" and \"slicing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Selecting Data Using Columns\n",
    "\n",
    "We use square brackets [] to select a subset of a Python object.  For example, I can select just my fips code from the mobility_df dataframe by name.\n",
    "\n",
    "You can also call an attribute of a dataframe by using the \"dataframename.attribute\" syntax.  Note that this doesn't work if you have spaces in your column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the codes in the next two cells do the same thing.\n",
    "mobility_df[\"fips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_df.fips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also assign an object to this function.  For example, I can call this object fips_column. What type of object is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_column=mobility_df[\"fips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the command for identifying the type of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we use double brackets? \n",
    "#Can you see what is different from the code below to the code above? \n",
    "\n",
    "fips_column=mobility_df[[\"fips\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we tried to do last week, by selecting a few variables. But our code got us a warning message when we created a new variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mistakenly thought the code created a new copy of the mobility_df DataFrame object. However, using the = operator in a simple statement y = x does not create a copy of our DataFrame. Instead, y = x creates a new object y that references the same object that x refers to. To state this another way, there is only one object (the DataFrame), and both x and y refer to it.\n",
    "\n",
    "An easy way to fix this is to make a copy of our dataframe. It really is like making a copy of your Excel worksheet and then just keeping some of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mobility_df=mobility_df[[\"fips\",\"tot\", \"owner\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore what's in \"new_mobility_df\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should be able to make new variables without a data error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_mobility_df[\"pct_owner\"]=new_mobility_df[\"owner\"]/new_mobility_df[\"tot\"]*100\n",
    "new_mobility_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Selecting Data with Pandas\n",
    "\n",
    "Ok!  Now we're getting somewhere.  What if we want to subset our data using rows, for example, making a separate dataframe for the county rows or selecting just our case study census tracts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s remind ourselves that Python uses 0-based indexing. This means that the first row in a dataframe (really, in any object or series) is located at position 0.  This is different from Excel which starts with the number 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing using the [ ] operator selects a set of rows and/or columns from a DataFrame. To slice out a set of rows, you use the following syntax: ***data[start position:stop position]***. When slicing in pandas the start bound is included in the output. The stop bound is one step BEYOND the row you want to select. So if you want to select rows 0, 1 and 2 your code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mobility_df[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can become more specific and select our data by columns and rows with the functions ***.loc*** and ***.iloc***. ***.loc*** is label-based, which means that you have to specify rows and columns based on their row and column labels (it stands for \"location\"). ***.iloc*** is integer index based, so you have to specify rows and columns by their integer index like you did in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that in this dataset, our row \"labels\" are the index numbers, so in this case,\n",
    "#for rows, .loc and .iloc look the same\n",
    "new_mobility_df.loc[0, \"tot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mobility_df.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, try and call up pct owners in San Francisco county (fips = 06075), \n",
    "# using both the .loc and .iloc commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also include ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mobility_df.loc[[0, 10], \"tot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mobility_df.loc[0:10, [\"tot\", \"owner\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, try and call the first three columns and the first 10 rows using iloc instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Working with Conditional Statements\n",
    "\n",
    "When we start working with our data, we want to treat our counties separately from our census tracts, otherwise, we might accidentally add the county values to our calculations.  Let's create a new dataframe with just our county data, and drop the county rows from our datasheet.  We do this with a \"conditional\" statement.  As a reminder, conditional operators are:\n",
    "\n",
    "**< less than**\n",
    "   \n",
    "**<= less than or equivalent to**\n",
    "   \n",
    "**> more than**\n",
    "   \n",
    "**>= more than or equivalent to**\n",
    "   \n",
    "**== equivalent to**\n",
    "\n",
    "**!= not equivalent to**\n",
    "\n",
    "If we want to combine more than one conditional statement, we'll use\n",
    "\n",
    "**& for \"and\"**\n",
    "\n",
    "**| for \"or\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I want to create a new dataframe, that includes just the data for the four Bay Area Counties\n",
    "counties=mobility_df[(mobility_df.fips==\"06001\") \n",
    "                         | (mobility_df.fips==\"06013\")\n",
    "                         | (mobility_df.fips==\"06075\")\n",
    "                         | (mobility_df.fips==\"06077\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now going to drop my county rows from my mobility_df dataframe, so I can calcuate statistics for census tracts without inadvertantly adding in the county data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping county rows.  There's lots of ways to do this, and we'll get more sophisticated\n",
    "# later, but for now, we're just going to tell it to drop the first four rows.\n",
    "# We have to use \"inplace=True\" because otherwise Python won't overwrite the original dataframe.\n",
    "mobility_df.drop([0, 1, 2, 3], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregating MOEs and Testing for Statistical Significance\n",
    "\n",
    "Now, we're going to go through the same steps as we did in Excel to calculate values for our case study, and compare them to the county.  I want to know whether the percent of people who lived in the same house a year ago in Downtown Oakland is the same or different from the percent of the population who lived in the same house one year ago in Alameda County. I also want to see what share of people \"in-migrated\" to my neighborhood, by adding up the number of people who moved in from out of the county, out of the state, or from abroad.\n",
    "\n",
    "Create a markdown cell under this one to write out the steps we did in Excel when we wanted to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  Create a Dummy Variable\n",
    "\n",
    "First, we need to create a dummy variable, so we can calculate the statistics for our case study.  We're going to use a conditional statement again, but this time, we're going to use our numpy library to assign a 1 to every tract that meets the condition, and 0 to every tract where the conditions aren't met.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_df[\"downtown_oakland\"]=np.where(((mobility_df[\"fips\"]==\"06001402800\") \n",
    "                         | (mobility_df[\"fips\"]==\"06001402900\")\n",
    "                         | (mobility_df[\"fips\"]==\"06001403000\")\n",
    "                         | (mobility_df[\"fips\"]==\"06001403100\")\n",
    "                         | (mobility_df[\"fips\"]==\"06001403300\")\n",
    "                         | (mobility_df[\"fips\"]==\"06001983200\")), 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test to see if it worked, printing just the fips code and the downtown_oakland variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell, create a dummy for your census tracts.  We've included Alameda, Contra Costa, \n",
    "# San Francisco, and San Joaquin data in the datasheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Aggregating Totals Across Rows\n",
    "\n",
    "Luckily, I don't need to aggregate any data for the people who lived in the same house - that's just one column of data.  But I do for my \"in-migrant\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running my column names here makes it so I don't have to scroll up to see my names.\n",
    "print(mobility_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For my \"different county or state\" I need to first aggregate my \"columns\".  Try it here.\n",
    "# IMPORTANT best practice: always print your input and output variables to check that your code worked right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I have to aggregate my MOEs. Square root sum of squares, right?  For square root, I have to call numpy.\n",
    "# Try it yourself here.  The code for square root is np.sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  Aggregating the Data for Our Case Study Community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dummy variable, and our \"summed\" variables across columns, we can quickly summarize the data for our case study using Pandas “groupby()” functionality. Groupby essentially splits the data into different groups depending on a variable of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code tells us to calculate the total population for our case study\n",
    "\n",
    "mobility_df.groupby(\"downtown_oakland\").tot.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try calculating the average population for downtown oakland yourself.  How about the standard deviation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What about the total number of people who are inmigrants?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a lot of the calculations using the built in calculations, but if we want to aggregate our MOEs, we need to use the square root sum of squares formula.  This is going to require a bit more work.\n",
    "\n",
    "In the code below, I create a \"dictionary\" that assigns the \"aggregation\" functions I want to apply to each column. Sum is easy - I just want to \"sum\" the \"tot\" column.  For the MOE aggregation, I'm going to invoke a \"lambda\" function x, which is a more complicated mathematical operation.  \n",
    "\n",
    "I'm going to assign a column function to get the \"total\" and \"total_moe\" for each variable I need to answer my reserach question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I am creating a \"dictionary\" that tells python what to do\n",
    "aggregations = {\"tot\": \"sum\", \"tot_moe\" :lambda x: np.sqrt(np.sum(x)**2),\n",
    "               \"tot_samehouse\": \"sum\", \"tot_samehouse_moe\" :lambda x: np.sqrt(np.sum(x)**2),\n",
    "               \"tot_inmigrant\": \"sum\", \"tot_inmigrant_moe\": lambda x: np.sqrt(np.sum(x)**2)}\n",
    "#Here I am creating a new case study dataframe, where I say \"agg\" the data using the \"groupby\" \n",
    "# dummy for \"downtown_oakland\", using the \"aggregations\" dictionary listed above\n",
    "casestudy=mobility_df.groupby(\"downtown_oakland\").agg(aggregations)\n",
    "# Now just print my new dataframe so I can make sure it worked!\n",
    "casestudy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Calculating A Derived Proportion and MOE\n",
    "\n",
    "Almost there!  Now, I'll calculate the percent that live in the same house, and the percent inmigrants, and the associated MOEs.  This code is starting to look familiar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code calculates my variables of interest and their MOEs. \n",
    "# Notice how I have to call on the numpy library to do the square root.\n",
    "# Also notice that I don't multiply by 100 to get a percentage - I want to leave it as a decimal to calculate my MOEs.\n",
    "casestudy[\"pct_samehouse\"]=casestudy[\"tot_samehouse\"]/casestudy[\"tot\"]\n",
    "casestudy[\"pct_samehouse_moe\"]=np.sqrt(casestudy['tot_samehouse_moe']**2 - \n",
    "                                          (casestudy['pct_samehouse']**2 * \n",
    "                                           casestudy['tot_moe']**2)) / casestudy['tot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  You try it for the derived proportion of in-migrants!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 OPTIONAL: Testing for Statistical Signficance\n",
    "\n",
    "Let's say I want to test to see if the difference between my downtown oakland case study community (downtown_oakland=1) is different from the rest of the Bay Area (downtown_oakland=0).  In practice, we wouldn't do this - we would get Alameda county or the Oakland City estimates. But to keep things simple to start, how might you enter the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_CRIT=1.645\n",
    "x1 = .87\n",
    "x2 = .78\n",
    "\n",
    "se_x1 = .05/Z_CRIT\n",
    "se_x2 = .07/Z_CRIT\n",
    "\n",
    "z=(x1 - x2)/np.sqrt(se_x1**2 + se_x2**2)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant!!!  What about for in-migrants?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Weekend Homework\n",
    "\n",
    "The only way to get better at Python is to practice. The goal is to just keep practicing writing code and making errors, so you get more familiar with the syntax.  Just like learning a language, it feels unfamiliar at first and you'll make a lot of mistakes!\n",
    "\n",
    "If you feel like you want a simpler practice exercise, we have provided a different dataset and \"assignment\" notebook to give you an opportunity to keep practicing the skills we want you to master by next Thursday's lab.  Make sure you set aside time to work on it!\n",
    "\n",
    "If you're feeling more confident, try to replicate this notebook on your own using one of your own ACS files - or a new one! Try and do it for two different time periods so you can see the changes in your case study community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
