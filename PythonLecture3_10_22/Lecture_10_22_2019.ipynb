{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Working with Disaggregate Data\n",
    "\n",
    "Today, we're going to play with characterizing and cleaning disaggregate data, and getting used to working with different types of variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call our libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datascience import *\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is my code for reading in the ADULT CHIS data - I have provided everyone with an extract of the data \n",
    "# so you don't need to run this cell today, but it might be helpful if you decide to work with the CHIS data\n",
    "# for Assignment 2\n",
    "\n",
    "#chis_df=pd.read_stata(\"ADULT.dta\", columns = [\"ac11\",\"povll\", \"ab1\", \"racedf_p1\", \"ak28\", \"ak25\", \"ak10_p\", \"ak22_p1\"])\n",
    "\n",
    "# This code exports my extract to a csv\n",
    "#chis_df.to_csv(\"chis_extract.csv\", index=False)\n",
    "\n",
    "# And this should look familiar - I'm reading the csv file I've created above.\n",
    "chis_df=pd.read_csv(\"chis_extract.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type in the code to look at the data\n",
    "chis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Renaming the variables\n",
    "\n",
    "When you go to work with a new dataset, you need to spend some time with the codebook, figuring out what variables you want to analyse, and what each \"code\" means.  Here, I've renamed the variables for you so we can get to the fun stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df.rename(columns={\"ac11\":\"number_sodas\",\"povll\":\"poverty_line\",\n",
    "\"ab1\":\"health\",\n",
    "\"racedf_p1\":\"race_eth\",\n",
    "\"ak28\":\"feel_safe\",\n",
    "\"ak25\":\"tenure\",\n",
    "\"ak10_p\":\"earnings\",\n",
    "\"ak22_p1\":\"hh_income\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Assessing and Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what data types are our variables?  \n",
    "chis_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change the number of sodas from a \"category\" to a float.\n",
    "\n",
    "chis_df[\"number_sodas\"]=chis_df[\"number_sodas\"].astype(float)\n",
    "chis_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now try it for earnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What happened?  The \"inapplicable\" entry is a string, which Python refused to convert to a number.  \n",
    "#We can force it to by telling it to \"coerce\" the new data type when it finds an error.\n",
    "\n",
    "chis_df[\"earnings\"]=pd.to_numeric(chis_df[\"earnings\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your data types and look at your data again.  What has Python entered for the \"Inapplicable\" missing value? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We often want to know how many variables have missing data, and how many observation are missing.\n",
    "chis_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  Understanding Numeric Variables\n",
    "\n",
    "There are lots of different ways to explore numeric variables.  The two codes below are ways I always check my variables before I start any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df[\"number_sodas\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df[\"number_sodas\"].quantile([.1, .25, .50, .75, .99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Exploring Categorical Data\n",
    "\n",
    "To explore categorical, or binary, data, we use what's called a frequency table instead.  Again, there's lots of different ways to get to the same answer, but here are some useful codes for understanding categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df[\"tenure\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df[\"tenure\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df[\"tenure\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Panda \"crosstab\" function is also helpful, especially when we want to create a frequency table with two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(chis_df[\"tenure\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Cleaning up our Categorical Data\n",
    "\n",
    "In looking at the tenure variable, it's pretty clear we have a lot of \"missing\" values coded as other things.  What I really want to understand is the difference between renters and owners - the other categories are not as meaningful.  I don't really want to drop these rows, since the people who \"REFUSED\" or selected \"OTHER ARRANGEMENT\" may have valuable answers to other questions.  What I'd like to do is create a new dummy variable, which is equal to 1 for owners, and 0 for renters, and missing for every other category.  I'm going to use a \"dictionary\" file to assign these new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df[\"own_dv\"]=chis_df[\"tenure\"].map({\"OWN\":1, \"RENT\":0, \"REFUSED\":np.nan, \"NOT ASCERTAINED\":np.nan, \"DON'T KNOW\": np.nan, \"OTHER ARRANGEMENT\": np.nan})\n",
    "pd.crosstab(chis_df[\"own_dv\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also has a \"get dummies\" function.  It is a faster way of creating dummies, especially if you have a variable with lots of categories.  But how does it differ from the code above?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=pd.get_dummies(chis_df, columns=[\"tenure\"])\n",
    "newdata.tenure_OWN.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be very careful in how you use functions like this - if you have \"messy\" real life data, you could be creating your dummies in such a way that will profoundly affect your results.  I highly recommend using the more thoughtful process outlined above, at least until you get more familiar with inferential statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6  Grouping Data by Categories\n",
    "\n",
    "Often, we want to know differences in outcomes across groups - let's see whether owners or renters drink more sodas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df[\"number_sodas\"].groupby(chis_df[\"own_dv\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df[\"number_sodas\"].groupby(chis_df[\"own_dv\"]).agg(['min','max','mean', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df.groupby(['own_dv']).agg({'number_sodas': 'mean',\n",
    "                                  'earnings' : 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Two Variable Frequency Tables\n",
    "\n",
    "If we want to explore two categorical variables, we need to rely on the Panda crosstab function.  We specify which variable we want along the rows (our \"index\" variable) and which variable we want along our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_poverty=pd.crosstab(index=chis_df[\"poverty_line\"], columns=chis_df[\"health\"])\n",
    "health_poverty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If we want Python to give us row and column totals, we specify that using\n",
    "# the \"Margins=true\" option within the crosstab function.\n",
    "health_poverty=pd.crosstab(index=chis_df[\"poverty_line\"], columns=chis_df[\"health\"], margins=True)\n",
    "health_poverty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the percents by asking Python to normalize the data\n",
    "health_poverty=pd.crosstab(index=chis_df[\"poverty_line\"], columns=chis_df[\"health\"], margins=True, normalize=\"index\")\n",
    "health_poverty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try it here, this time \"normalizing\" (getting the percents for) the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Next Steps\n",
    "\n",
    "Work on answering the questions in the lab sheet below with a group of your classmates.  The goal for Thursday's lab is to apply these same Python skills to your own dataset, so it's worth practicing and getting comfortable with these commands.  Again, you don't have to memorize them, just know what each of them do and or feel empowered to copy, paste and run and see what happens!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
